---
_blueprint:
  content: "Start talking research methodology to journalists, and many will run\r\nscreaming
    from the room. To be fair, the same might be said of some\r\nacademics working
    outside the bounds of history, social or pure science\r\ndepartments. Yet, however
    technical and dry it seems, researchers\r\nworking in methodology-driven disciplines
    know that valid methods arenâ\x80\x99t\r\na detail, but at the heart of the most
    important question to be asked of\r\nany study: can you trust the results?\r\n\r\nBut
    while academics are under increasing pressure to promote and discuss\r\ntheir
    research findings in the media, they often face time-pressed\r\nreporters who
    know little about what it takes to conduct valid research.\r\nSometimes, they
    seem to not want to know, given that knowledge that a\r\npiece of research is
    rubbish does them out of a story.\r\n\r\nThe uncritical reporting of the Southern
    Cross Bioethics Instituteâ\x80\x99s\r\nlaunch of their report on abortion is a
    case in point. While the media\r\nwidely reported the â\x80\x9Cfindingsâ\x80\x9D
    the Institute provided at their launch\r\nand via a summary on their website,
    journalists were not given\r\ninformation on the studyâ\x80\x99s conduct (how
    subjects were selected, the\r\nquestions they were asked, the statistical tests
    used to crunch the\r\nnumbers), and who funded it. Academics and activists attempting
    to do\r\ntheir homework before providing the media with requested commentary\r\ncouldnâ\x80\x99t
    get obtain such information either. When one activist asked,\r\nthe PR consultants
    fielding enquiries initially promised it by email,\r\nonly to renege later that
    day with a garbled excuse about only having\r\npermission to distribute the report
    - after payment via the website -\r\nthrough the post. A high-profile statistician
    who actually teaches\r\nresearch methods to university students was flatly refused
    a copy of the\r\nquestions and told they wouldnâ\x80\x99t be in the report either
    not to bother\r\nwaiting by her mailbox, as the written report also didnâ\x80\x99t
    reveal them.\r\n\r\nDespite this, the report claims to â\x80\x9Creflect the attitudes
    to abortion of\r\nAustralians generallyâ\x80\x9D, though it fails to adequately
    explain why if\r\nthis is so, the results are so discordant with polls by reputable\r\ninstitutions
    like the Australian National University, which does make\r\nits research questions
    and methods available for public scrutiny.\r\n\r\nThe peer review process ensures
    that the value of data - and the\r\nconclusions drawn from it - are never taken
    at face value. A bottom-line\r\nrequirement for the publication of academic articles
    and books is the\r\nprovision of extensive information on how data was collected
    and\r\ncrunched. Disclosure of who funded the research has, thankfully, become\r\nde
    rigour. This is because academics are trained to be sceptics, and are\r\nwell
    aware of the biases that can be introduced through the way in which\r\nresearch
    is conduced and funded, not just how the results are analysed.\r\n\r\nJournalists
    are trained to be sceptics too, and the exercise of their\r\ncritical faculties
    regularly protects the public from swallowing what\r\nthose controlling the agenda
    want us to see and believe. During the US\r\nelection, Australian journos reported
    extensively and well on the way\r\nâ\x80\x9Cpush-pollingâ\x80\x9D conducted during
    the US election distorted the data\r\nproduced. Yet as the Southern Cross Bioethics
    Institute saga makes\r\nclear, a lack of journalistic scepticism about what constitutes
    valid\r\nresearch - coupled with organisational competitiveness and tight\r\ndeadlines
    - can have significant outcome for the nature and quality of\r\npublic debate.\r\n\r\nHow
    might things go better next time? Firstly, media academics and\r\njournalists
    need to debate amongst themselves, in peer-reviewed journals\r\nand magazines
    like the Walkley, the precise nature of their\r\nresponsibilities when it comes
    to publicising research results. Is it\r\nthe mediaâ\x80\x99s role to assess the
    quality of data, and source the money\r\nbehind it, before placing it before the
    public? Or is the provision of\r\nspace for critical commentary on a studyâ\x80\x99s
    methods and findings all that\r\ncan reasonably be required?\r\n\r\nFor their
    part, academics must insist in their interactions with\r\njournalists that methodology
    does matter, and provide clear and\r\nconvincing examples of why this is so. Journalists
    are in the business\r\nof asking questions. Itâ\x80\x99s not hard for them to
    understand that the way\r\nresearchers ask them impacts on the responses they
    get. Nor should it be\r\nhard to convince them that data generated from surveys
    of large,\r\nrandomly selected adults of all ages and both genders, is going to
    be\r\nmore robust than that obtained from 12 elderly members of a church group\r\nplus
    their relatives and friends. This isnâ\x80\x99t to say that the latter\r\ntechnique
    canâ\x80\x99t be used, but that it canâ\x80\x99t be used if the researcher\r\nwants
    to claim the results represent the views of all Australians.\r\n\r\nAcademics
    must also develop and market short, on-site courses for media\r\npractitioners
    that elucidate the basic methodological requirements of\r\ngood research, and
    the basic questions journalists must ask - and have\r\nanswered - before any story
    goes to press or to air."
  content_markup: "<p>Start talking research methodology to journalists, and many
    will run\nscreaming from the room. To be fair, the same might be said of some\nacademics
    working outside the bounds of history, social or pure science\ndepartments. Yet,
    however technical and dry it seems, researchers\nworking in methodology-driven
    disciplines know that valid methods arenâ\x80\x99t\na detail, but at the heart
    of the most important question to be asked of\nany study: can you trust the results?</p>\n\n<p>But
    while academics are under increasing pressure to promote and discuss\ntheir research
    findings in the media, they often face time-pressed\nreporters who know little
    about what it takes to conduct valid research.\nSometimes, they seem to not want
    to know, given that knowledge that a\npiece of research is rubbish does them out
    of a story.</p>\n\n<p>The uncritical reporting of the Southern Cross Bioethics
    Instituteâ\x80\x99s\nlaunch of their report on abortion is a case in point. While
    the media\nwidely reported the â\x80\x9Cfindingsâ\x80\x9D the Institute provided
    at their launch\nand via a summary on their website, journalists were not given\ninformation
    on the studyâ\x80\x99s conduct (how subjects were selected, the\nquestions they
    were asked, the statistical tests used to crunch the\nnumbers), and who funded
    it. Academics and activists attempting to do\ntheir homework before providing
    the media with requested commentary\ncouldnâ\x80\x99t get obtain such information
    either. When one activist asked,\nthe PR consultants fielding enquiries initially
    promised it by email,\nonly to renege later that day with a garbled excuse about
    only having\npermission to distribute the report &ndash; after payment via the
    website &ndash;\nthrough the post. A high-profile statistician who actually teaches\nresearch
    methods to university students was flatly refused a copy of the\nquestions and
    told they wouldnâ\x80\x99t be in the report either not to bother\nwaiting by her
    mailbox, as the written report also didnâ\x80\x99t reveal them.</p>\n\n<p>Despite
    this, the report claims to â\x80\x9Creflect the attitudes to abortion of\nAustralians
    generallyâ\x80\x9D, though it fails to adequately explain why if\nthis is so,
    the results are so discordant with polls by reputable\ninstitutions like the Australian
    National University, which does make\nits research questions and methods available
    for public scrutiny.</p>\n\n<p>The peer review process ensures that the value
    of data &ndash; and the\nconclusions drawn from it &ndash; are never taken at
    face value. A bottom-line\nrequirement for the publication of academic articles
    and books is the\nprovision of extensive information on how data was collected
    and\ncrunched. Disclosure of who funded the research has, thankfully, become\nde
    rigour. This is because academics are trained to be sceptics, and are\nwell aware
    of the biases that can be introduced through the way in which\nresearch is conduced
    and funded, not just how the results are analysed.</p>\n\n<p>Journalists are trained
    to be sceptics too, and the exercise of their\ncritical faculties regularly protects
    the public from swallowing what\nthose controlling the agenda want us to see and
    believe. During the US\nelection, Australian journos reported extensively and
    well on the way\nâ\x80\x9Cpush-pollingâ\x80\x9D conducted during the US election
    distorted the data\nproduced. Yet as the Southern Cross Bioethics Institute saga
    makes\nclear, a lack of journalistic scepticism about what constitutes valid\nresearch
    &ndash; coupled with organisational competitiveness and tight\ndeadlines &ndash;
    can have significant outcome for the nature and quality of\npublic debate.</p>\n\n<p>How
    might things go better next time? Firstly, media academics and\njournalists need
    to debate amongst themselves, in peer-reviewed journals\nand magazines like the
    Walkley, the precise nature of their\nresponsibilities when it comes to publicising
    research results. Is it\nthe mediaâ\x80\x99s role to assess the quality of data,
    and source the money\nbehind it, before placing it before the public? Or is the
    provision of\nspace for critical commentary on a studyâ\x80\x99s methods and findings
    all that\ncan reasonably be required?</p>\n\n<p>For their part, academics must
    insist in their interactions with\njournalists that methodology does matter, and
    provide clear and\nconvincing examples of why this is so. Journalists are in the
    business\nof asking questions. Itâ\x80\x99s not hard for them to understand that
    the way\nresearchers ask them impacts on the responses they get. Nor should it
    be\nhard to convince them that data generated from surveys of large,\nrandomly
    selected adults of all ages and both genders, is going to be\nmore robust than
    that obtained from 12 elderly members of a church group\nplus their relatives
    and friends. This isnâ\x80\x99t to say that the latter\ntechnique canâ\x80\x99t
    be used, but that it canâ\x80\x99t be used if the researcher\nwants to claim the
    results represent the views of all Australians.</p>\n\n<p>Academics must also
    develop and market short, on-site courses for media\npractitioners that elucidate
    the basic methodological requirements of\ngood research, and the basic questions
    journalists must ask &ndash; and have\nanswered &ndash; before any story goes
    to press or to air.</p>\n"
  created_on: 2011-03-23 12:12:08
  excerpt: ''
  excerpt_markup: ''
  id: '629'
  link: ''
  page_id: '596'
  publish_on: 2005-05-16
  site_id: '15'
  slug: madness-in-the-method
  title: Madness in the Method
  updated_on: 2011-03-25 17:01:33
assets: ~
excerpt: ''
published: 2005-05-16
tags: ~
title: Madness in the Method
--- "Start talking research methodology to journalists, and many will run\r\nscreaming
  from the room. To be fair, the same might be said of some\r\nacademics working outside
  the bounds of history, social or pure science\r\ndepartments. Yet, however technical
  and dry it seems, researchers\r\nworking in methodology-driven disciplines know
  that valid methods arenâ\x80\x99t\r\na detail, but at the heart of the most important
  question to be asked of\r\nany study: can you trust the results?\r\n\r\nBut while
  academics are under increasing pressure to promote and discuss\r\ntheir research
  findings in the media, they often face time-pressed\r\nreporters who know little
  about what it takes to conduct valid research.\r\nSometimes, they seem to not want
  to know, given that knowledge that a\r\npiece of research is rubbish does them out
  of a story.\r\n\r\nThe uncritical reporting of the Southern Cross Bioethics Instituteâ\x80\x99s\r\nlaunch
  of their report on abortion is a case in point. While the media\r\nwidely reported
  the â\x80\x9Cfindingsâ\x80\x9D the Institute provided at their launch\r\nand via
  a summary on their website, journalists were not given\r\ninformation on the studyâ\x80\x99s
  conduct (how subjects were selected, the\r\nquestions they were asked, the statistical
  tests used to crunch the\r\nnumbers), and who funded it. Academics and activists
  attempting to do\r\ntheir homework before providing the media with requested commentary\r\ncouldnâ\x80\x99t
  get obtain such information either. When one activist asked,\r\nthe PR consultants
  fielding enquiries initially promised it by email,\r\nonly to renege later that
  day with a garbled excuse about only having\r\npermission to distribute the report
  - after payment via the website -\r\nthrough the post. A high-profile statistician
  who actually teaches\r\nresearch methods to university students was flatly refused
  a copy of the\r\nquestions and told they wouldnâ\x80\x99t be in the report either
  not to bother\r\nwaiting by her mailbox, as the written report also didnâ\x80\x99t
  reveal them.\r\n\r\nDespite this, the report claims to â\x80\x9Creflect the attitudes
  to abortion of\r\nAustralians generallyâ\x80\x9D, though it fails to adequately
  explain why if\r\nthis is so, the results are so discordant with polls by reputable\r\ninstitutions
  like the Australian National University, which does make\r\nits research questions
  and methods available for public scrutiny.\r\n\r\nThe peer review process ensures
  that the value of data - and the\r\nconclusions drawn from it - are never taken
  at face value. A bottom-line\r\nrequirement for the publication of academic articles
  and books is the\r\nprovision of extensive information on how data was collected
  and\r\ncrunched. Disclosure of who funded the research has, thankfully, become\r\nde
  rigour. This is because academics are trained to be sceptics, and are\r\nwell aware
  of the biases that can be introduced through the way in which\r\nresearch is conduced
  and funded, not just how the results are analysed.\r\n\r\nJournalists are trained
  to be sceptics too, and the exercise of their\r\ncritical faculties regularly protects
  the public from swallowing what\r\nthose controlling the agenda want us to see and
  believe. During the US\r\nelection, Australian journos reported extensively and
  well on the way\r\nâ\x80\x9Cpush-pollingâ\x80\x9D conducted during the US election
  distorted the data\r\nproduced. Yet as the Southern Cross Bioethics Institute saga
  makes\r\nclear, a lack of journalistic scepticism about what constitutes valid\r\nresearch
  - coupled with organisational competitiveness and tight\r\ndeadlines - can have
  significant outcome for the nature and quality of\r\npublic debate.\r\n\r\nHow might
  things go better next time? Firstly, media academics and\r\njournalists need to
  debate amongst themselves, in peer-reviewed journals\r\nand magazines like the Walkley,
  the precise nature of their\r\nresponsibilities when it comes to publicising research
  results. Is it\r\nthe mediaâ\x80\x99s role to assess the quality of data, and source
  the money\r\nbehind it, before placing it before the public? Or is the provision
  of\r\nspace for critical commentary on a studyâ\x80\x99s methods and findings all
  that\r\ncan reasonably be required?\r\n\r\nFor their part, academics must insist
  in their interactions with\r\njournalists that methodology does matter, and provide
  clear and\r\nconvincing examples of why this is so. Journalists are in the business\r\nof
  asking questions. Itâ\x80\x99s not hard for them to understand that the way\r\nresearchers
  ask them impacts on the responses they get. Nor should it be\r\nhard to convince
  them that data generated from surveys of large,\r\nrandomly selected adults of all
  ages and both genders, is going to be\r\nmore robust than that obtained from 12
  elderly members of a church group\r\nplus their relatives and friends. This isnâ\x80\x99t
  to say that the latter\r\ntechnique canâ\x80\x99t be used, but that it canâ\x80\x99t
  be used if the researcher\r\nwants to claim the results represent the views of all
  Australians.\r\n\r\nAcademics must also develop and market short, on-site courses
  for media\r\npractitioners that elucidate the basic methodological requirements
  of\r\ngood research, and the basic questions journalists must ask - and have\r\nanswered
  - before any story goes to press or to air."
